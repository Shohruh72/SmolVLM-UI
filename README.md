# ðŸ¦™ SmolVLM Realtime Inference UI

A lightweight, blazing-fast visual interface for **SmolVLM** models â€”  
run real-time vision inference from your **webcam**, **video files**, or **images**, right in your browser.


## ðŸ–¼ï¸ Interface Preview

| Webcam Input        | Video File         | Image Upload        |
| ------------------- | ------------------ | ------------------- |
| ![](demo/webcam.png) | ![](demo/video.png) | ![](demo/image.png) |


---

## ðŸš€ Features

#### - ðŸ“¸ Webcam, ðŸ“¹ Video File, ðŸ–¼ï¸ Image Upload support
#### - ðŸ”„ Easy model switching (SmolVLM / SmolVLM2)
#### - âš¡ Lightweight UI â€” pure HTML + JS
#### - ðŸ§  Powered by `llama.cpp` + GGUF models
#### - âœ… Offline & local-server ready

---

## âš™ï¸ Setup (First Time Only)

```bash
git clone https://github.com/Shohruh72/SmolVLM-UI.git
cd SmolVLM-UI
chmod +x setup.sh run_server.sh
./setup.sh   # builds and selects your model
```
## â–¶ï¸ Inference (Anytime After)
```bash
./run_server.sh   # no rebuild, just launch
```
## Open the UI
Then open index.html in your browser and start prompting!

> ðŸ’¡ **Tip:** After youâ€™ve run `./setup.sh` once, you only need to use `./run_server.sh` on subsequent runs.

## ðŸ“¦ Supported Models
```
* ggml-org/SmolVLM-Instruct-GGUF
* ggml-org/SmolVLM-256M-Instruct-GGUF
* ggml-org/SmolVLM-500M-Instruct-GGUF
* ggml-org/SmolVLM2-2.2B-Instruct-GGUF
* ggml-org/SmolVLM2-256M-Video-Instruct-GGUF
* ggml-org/SmolVLM2-500M-Video-Instruct-GGUF
```

## Reference

- Inspired by [http://github.com/ngxson/smolvlm-realtime-webcam](http://github.com/ngxson/smolvlm-realtime-webcam) repository.


